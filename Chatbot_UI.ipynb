{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf_VHIcKr-yD"
      },
      "source": [
        "## Pure gemini model for input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "zUfC2U3pkxL9",
        "outputId": "49b9ac70-3d17-4112-95c7-fb50acc650f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m315.2/315.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hHello there! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U google-generativeai gradio\n",
        "import google.generativeai as genai, os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"  # paste your key here just for testing\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# sanity check\n",
        "print(genai.GenerativeModel(\"gemini-2.5-pro\").generate_content(\"hi\").text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "hOc93SZcKmiQ",
        "outputId": "e4381e63-7c6f-4801-9fe1-f5a6a666a3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n",
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n",
            "Launching app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://22790c7423ffcb6f51.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://22790c7423ffcb6f51.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 2.041 seconds.\n",
            "DEBUG:jieba:Loading model cost 2.041 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://22790c7423ffcb6f51.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\n",
        "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\n",
        "            \"ÊòØ\",\"ÁöÑ\",\"‰∫Ü\",\"Âíå\",\"Âú®\",\"Â∞±\",\"‰πü\",\"ÈÉΩ\"\n",
        "        ]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "\n",
        "    # Chinese\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging (log BOTH user and assistant text)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"timestamp\", \"prob\", \"label\", \"user_text\", \"assistant_text\"])\n",
        "\n",
        "def log_turn_to_csv(user_text: str, prob: float, is_stress: bool,\n",
        "                    assistant_text: str, path: str = LOG_CSV):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "    label = \"STRESS\" if is_stress else \"CALM\"\n",
        "\n",
        "    # Keep one-row-per-turn\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([ts, round(prob, 4), label, safe_user_text, safe_assistant_text])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# This prompt is for when the user IS stressful\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "\"\"\"\n",
        "\n",
        "# This prompt is for when the user is NOT stressful\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 7) Gradio app (messages-only for Gradio 6)\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"\n",
        "    Gradio 6 Chatbot uses a list of dicts:\n",
        "      {\"role\": \"user\" | \"assistant\", \"content\": <str or other>}\n",
        "    Convert this into Gemini-style history with plain string parts.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        return []\n",
        "\n",
        "    gemini_history = []\n",
        "    for msg in chat_history:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        raw_content = msg.get(\"content\", \"\")\n",
        "\n",
        "        # Normalize to string\n",
        "        content = _content_to_str(raw_content)\n",
        "\n",
        "        # Strip router tag before sending to Gemini\n",
        "        if isinstance(content, str):\n",
        "            content = content.split(\"  [svc:\", 1)[0]\n",
        "\n",
        "        gemini_history.append({\n",
        "            \"role\": \"user\" if role == \"user\" else \"model\",\n",
        "            \"parts\": [content]\n",
        "        })\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    # Gradio passes messages-format history (list[dict]) or None\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        })\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # 1) Route via SVC\n",
        "    is_stress, prob = predict_with_prob(message)\n",
        "\n",
        "    tag = f\"[svc: {'STRESS' if is_stress else 'CALM'}; p={prob:.2f}; thr={_THR:.2f}]\"\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [svc: {'STRESS' if is_stress else 'CALM'}]\"\n",
        "\n",
        "    # 2) Build Gemini history from existing messages\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    chat_session = (model_stress if is_stress else model_calm).start_chat(history=gemini_history)\n",
        "\n",
        "    # 3) Call Gemini\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling Gemini: {e}\"\n",
        "\n",
        "    # 4) Append new turn in messages format\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "    # 5) Log turn INCLUDING assistant text\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            prob=prob,\n",
        "            is_stress=is_stress,\n",
        "            assistant_text=response_text,\n",
        "            path=LOG_CSV\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # Clear textbox, update chatbot\n",
        "    return \"\", chat_history\n",
        "\n",
        "def clear_chat():\n",
        "    # For messages format, just return empty list\n",
        "    return [], \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† Compassionate AI Guide\")\n",
        "    gr.Markdown(\"Each turn is routed by your SVC classifier. Decisions are logged to CSV.\")\n",
        "\n",
        "    # Chatbot uses messages format by default in Gradio 6\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    gr.Markdown(f\"**Logging to:** `{LOG_CSV}`\")\n",
        "\n",
        "    # Enter key submits\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "\n",
        "    # Send button submits\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "\n",
        "    # Clear button clears chat and textbox\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, theme=gr.themes.Soft(), debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbEl_fEl954k"
      },
      "source": [
        "# Judge model edition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CGpCVJS995o2",
        "outputId": "1d624c20-2a0d-40ac-8bae-63d244c8568e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n",
            "Launching app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a95b8f0dfb25c622a4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a95b8f0dfb25c622a4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.621 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.621 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0100, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1445 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0200, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1495 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0100, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1445 -> CALM\n",
            "[svc-v1] P(stress)=0.2789, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0100, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1445 -> CALM\n",
            "[svc-v1] P(stress)=0.2007, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0100, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1054 -> CALM\n",
            "[svc-v1] P(stress)=0.1964, thr=0.45 -> CALM\n",
            "[judge] P(stress)=0.0100, label=CALM, conf=high\n",
            "[fusion] final P(stress)=0.1032 -> CALM\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a95b8f0dfb25c622a4.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# With Gemini Judge fusion (50% SVC, 50% Judge)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib, json\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\n",
        "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\n",
        "            \"ÊòØ\",\"ÁöÑ\",\"‰∫Ü\",\"Âíå\",\"Âú®\",\"Â∞±\",\"‰πü\",\"ÈÉΩ\"\n",
        "        ]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "\n",
        "    # Chinese\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging (now logs classifier, judge, fusion, reasoning)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"timestamp\",\n",
        "                \"clf_prob\",\n",
        "                \"clf_label\",\n",
        "                \"judge_prob\",\n",
        "                \"judge_label\",\n",
        "                \"judge_confidence\",\n",
        "                \"judge_reasoning\",\n",
        "                \"final_prob\",\n",
        "                \"final_label\",\n",
        "                \"user_text\",\n",
        "                \"assistant_text\",\n",
        "            ])\n",
        "\n",
        "def log_turn_to_csv(\n",
        "    user_text: str,\n",
        "    clf_prob: float,\n",
        "    clf_label: str,\n",
        "    judge_prob: float,\n",
        "    judge_label: str,\n",
        "    judge_confidence: str,\n",
        "    judge_reasoning: str,\n",
        "    final_prob: float,\n",
        "    final_label: str,\n",
        "    assistant_text: str,\n",
        "    path: str = LOG_CSV,\n",
        "):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_judge_reason = judge_reasoning.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            ts,\n",
        "            round(clf_prob, 4),\n",
        "            clf_label,\n",
        "            round(judge_prob, 4),\n",
        "            judge_label,\n",
        "            judge_confidence,\n",
        "            safe_judge_reason,\n",
        "            round(final_prob, 4),\n",
        "            final_label,\n",
        "            safe_user_text,\n",
        "            safe_assistant_text,\n",
        "        ])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Prompts\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
        "You are a stress detection judge.\n",
        "Given a user's message and another model's probability P(stress), you estimate:\n",
        "- your own probability that the user is stressed,\n",
        "- a label (STRESS or CALM),\n",
        "- a confidence level,\n",
        "- and a short reasoning step by step.\n",
        "\n",
        "You MUST respond with a single line of valid JSON only, no extra text, with fields:\n",
        "{\n",
        "  \"judge_prob\": float between 0 and 1,\n",
        "  \"judge_label\": \"STRESS\" or \"CALM\",\n",
        "  \"confidence\": \"low\" or \"medium\" or \"high\",\n",
        "  \"reasoning\": \"short explanation within 50 words\"\n",
        "}\n",
        "\n",
        "Use the user message content as the main evidence. Use the classifier probability as a weak prior only.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    model_judge = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=JUDGE_SYSTEM_PROMPT\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "def call_judge_model(user_message: str, clf_prob: float, clf_label: str):\n",
        "    \"\"\"\n",
        "    Ask Gemini judge to evaluate stress with probability and reasoning.\n",
        "    Returns: (judge_prob, judge_label, confidence, reasoning)\n",
        "    Fallbacks to classifier if parsing fails.\n",
        "    \"\"\"\n",
        "    base_prompt = f\"\"\"\n",
        "user_message: {user_message}\n",
        "classifier_prob: {clf_prob:.4f}\n",
        "classifier_label: {clf_label}\n",
        "\"\"\"\n",
        "    try:\n",
        "        resp = model_judge.generate_content(base_prompt)\n",
        "        txt = resp.text.strip()\n",
        "        # Ensure we only parse the JSON part\n",
        "        # Try to find first '{' and last '}'\n",
        "        start = txt.find(\"{\")\n",
        "        end = txt.rfind(\"}\")\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            txt = txt[start:end+1]\n",
        "        data = json.loads(txt)\n",
        "\n",
        "        judge_prob = float(data.get(\"judge_prob\", clf_prob))\n",
        "        judge_prob = max(0.0, min(1.0, judge_prob))  # clamp\n",
        "        judge_label = data.get(\"judge_label\", \"STRESS\" if judge_prob >= 0.5 else \"CALM\").upper()\n",
        "        if judge_label not in (\"STRESS\", \"CALM\"):\n",
        "            judge_label = \"STRESS\" if judge_prob >= 0.5 else \"CALM\"\n",
        "        confidence = data.get(\"confidence\", \"medium\").lower()\n",
        "        if confidence not in (\"low\", \"medium\", \"high\"):\n",
        "            confidence = \"medium\"\n",
        "        reasoning = data.get(\"reasoning\", \"No reasoning provided.\")\n",
        "        return judge_prob, judge_label, confidence, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Judge parse error, falling back to classifier:\", e)\n",
        "        # Fallback to classifier opinion\n",
        "        judge_prob = clf_prob\n",
        "        judge_label = clf_label\n",
        "        confidence = \"low\"\n",
        "        reasoning = \"Judge model output could not be parsed, so the classifier probability was used.\"\n",
        "        return judge_prob, judge_label, confidence, reasoning\n",
        "\n",
        "# 7) Gradio app (messages-only for Gradio 6)\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"\n",
        "    Gradio 6 Chatbot uses a list of dicts:\n",
        "      {\"role\": \"user\" | \"assistant\", \"content\": <str or other>}\n",
        "    Convert this into Gemini-style history with plain string parts.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        return []\n",
        "\n",
        "    gemini_history = []\n",
        "    for msg in chat_history:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        raw_content = msg.get(\"content\", \"\")\n",
        "\n",
        "        # Normalize to string\n",
        "        content = _content_to_str(raw_content)\n",
        "\n",
        "        # Strip router tag before sending to Gemini\n",
        "        if isinstance(content, str):\n",
        "            content = content.split(\"  [svc:\", 1)[0]\n",
        "\n",
        "        gemini_history.append({\n",
        "            \"role\": \"user\" if role == \"user\" else \"model\",\n",
        "            \"parts\": [content]\n",
        "        })\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    # Gradio passes messages-format history (list[dict]) or None\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        })\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # 1) SVC classifier output\n",
        "    is_stress_clf, clf_prob = predict_with_prob(message)\n",
        "    clf_label = \"STRESS\" if is_stress_clf else \"CALM\"\n",
        "\n",
        "    # 2) Gemini judge output\n",
        "    judge_prob, judge_label, judge_conf, judge_reason = call_judge_model(\n",
        "        user_message=message,\n",
        "        clf_prob=clf_prob,\n",
        "        clf_label=clf_label\n",
        "    )\n",
        "    print(f\"[judge] P(stress)={judge_prob:.4f}, label={judge_label}, conf={judge_conf}\")\n",
        "\n",
        "    # 3) 50/50 fusion\n",
        "    final_prob = 0.5 * clf_prob + 0.5 * judge_prob\n",
        "    final_is_stress = final_prob >= 0.5\n",
        "    final_label = \"STRESS\" if final_is_stress else \"CALM\"\n",
        "    print(f\"[fusion] final P(stress)={final_prob:.4f} -> {final_label}\")\n",
        "\n",
        "    # 4) Tag for display\n",
        "    tag = (\n",
        "        f\"[svc={clf_label}; p_svc={clf_prob:.2f}; \"\n",
        "        f\"judge={judge_label}; p_judge={judge_prob:.2f}; \"\n",
        "        f\"p_final={final_prob:.2f}]\"\n",
        "    )\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [final: {final_label}]\"\n",
        "\n",
        "    # 5) Build Gemini chat history and route with fused label\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    routed_model = model_stress if final_is_stress else model_calm\n",
        "    chat_session = routed_model.start_chat(history=gemini_history)\n",
        "\n",
        "    # 6) Call conversation model\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini conversation error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling the conversation model: {e}\"\n",
        "\n",
        "    # 7) Append to UI history\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "    # 8) Log everything including judge reasoning\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            clf_prob=clf_prob,\n",
        "            clf_label=clf_label,\n",
        "            judge_prob=judge_prob,\n",
        "            judge_label=judge_label,\n",
        "            judge_confidence=judge_conf,\n",
        "            judge_reasoning=judge_reason,\n",
        "            final_prob=final_prob,\n",
        "            final_label=final_label,\n",
        "            assistant_text=response_text,\n",
        "            path=LOG_CSV,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # Clear textbox, update chatbot\n",
        "    return \"\", chat_history\n",
        "\n",
        "def clear_chat():\n",
        "    # For messages format, just return empty list\n",
        "    return [], \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† Compassionate AI Guide\")\n",
        "    gr.Markdown(\n",
        "        \"Each turn is classified by SVC, judged by Gemini, fused 50/50, \"\n",
        "        \"then routed to a calm or stress-support style. All decisions are logged.\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "        clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    gr.Markdown(f\"**Logging to:** `{LOG_CSV}`\")\n",
        "\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot])\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, theme=gr.themes.Soft(), debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "j9APj7qMNHyO",
        "outputId": "f07049c2-d7cc-4d02-c7ec-06b826b5c886"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Random Router Log CSV: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/random_router_logs_v2.csv\n",
            "Launching app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://48f5f49baf417bfcac.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://48f5f49baf417bfcac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Random Router + Gemini Chatbot with CSV Logging (turn by turn)\n",
        "# No classifier, no judge - random calm or stress routing\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, csv, random\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# Separate log file for the random router\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"random_router_logs_v2.csv\")\n",
        "\n",
        "print(\"Random Router Log CSV:\", LOG_CSV)\n",
        "\n",
        "# 2) CSV logging (routing only)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"timestamp\",\n",
        "                \"route_label\",       # STRESS or CALM\n",
        "                \"route_prob\",        # random score in [0, 1)\n",
        "                \"route_reasoning\",   # explanation of the routing decision\n",
        "                \"user_text\",\n",
        "                \"assistant_text\",\n",
        "            ])\n",
        "\n",
        "def log_turn_to_csv(\n",
        "    user_text: str,\n",
        "    route_label: str,\n",
        "    route_prob: float,\n",
        "    route_reasoning: str,\n",
        "    assistant_text: str,\n",
        "    path: str = LOG_CSV,\n",
        "):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_route_reason = route_reasoning.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            ts,\n",
        "            route_label,\n",
        "            round(route_prob, 4),\n",
        "            safe_route_reason,\n",
        "            safe_user_text,\n",
        "            safe_assistant_text,\n",
        "        ])\n",
        "\n",
        "# 3) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Prompts\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "* It is ok to briefly share why you suggested a certain exercise or perspective, in friendly plain language.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "* You can briefly explain why you think certain habits or ideas might help them keep this calm state.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 4) Gradio app\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"\n",
        "    Gradio 6 Chatbot uses a list of dicts:\n",
        "      {\"role\": \"user\" or \"assistant\", \"content\": <str or other>}\n",
        "    Convert this into Gemini-style history with plain string parts.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        return []\n",
        "\n",
        "    gemini_history = []\n",
        "    for msg in chat_history:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        raw_content = msg.get(\"content\", \"\")\n",
        "\n",
        "        content = _content_to_str(raw_content)\n",
        "\n",
        "        gemini_history.append({\n",
        "            \"role\": \"user\" if role == \"user\" else \"model\",\n",
        "            \"parts\": [content]\n",
        "        })\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True  # show random routing score in tag\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history, \"\"\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        error_msg = \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
        "        return \"\", chat_history, \"Routing information not available, models not loaded.\"\n",
        "\n",
        "    # 1) Random routing decision\n",
        "    route_prob = random.random()\n",
        "    route_is_stress = route_prob >= 0.5\n",
        "    route_label = \"STRESS\" if route_is_stress else \"CALM\"\n",
        "\n",
        "    # 2) Tag for display on user bubble\n",
        "    tag = f\"[route={route_label}; p_route={route_prob:.2f}]\"\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [route: {route_label}]\"\n",
        "\n",
        "    # 3) Build Gemini chat history and route\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    routed_model = model_stress if route_is_stress else model_calm\n",
        "    chat_session = routed_model.start_chat(history=gemini_history)\n",
        "\n",
        "    # 4) Call conversation model\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini conversation error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling the conversation model: {e}\"\n",
        "\n",
        "    # 5) Routing reasoning\n",
        "    style_word = \"supportive, stress-focused\" if route_is_stress else \"calm, positive\"\n",
        "    route_reasoning = (\n",
        "        f\"The router randomly chose a {style_word} style because the random score \"\n",
        "        f\"was {route_prob:.2f} with threshold 0.50, so the route label is {route_label}.\"\n",
        "    )\n",
        "\n",
        "    # Assistant message with embedded explanation\n",
        "    assistant_display = (\n",
        "        f\"{response_text}\\n\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"*Routing explanation:* {route_reasoning}\"\n",
        "    )\n",
        "\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_display})\n",
        "\n",
        "    # 6) Log routing and conversation\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            route_label=route_label,\n",
        "            route_prob=route_prob,\n",
        "            route_reasoning=route_reasoning,\n",
        "            assistant_text=assistant_display,\n",
        "            path=LOG_CSV,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # 7) Text for side panel\n",
        "    side_panel_text = (\n",
        "        f\"Route label: {route_label} (random score {route_prob:.2f})\\n\"\n",
        "        f\"Reasoning: {route_reasoning}\"\n",
        "    )\n",
        "\n",
        "    return \"\", chat_history, side_panel_text\n",
        "\n",
        "def clear_chat():\n",
        "    # Reset chatbot to initial greeting and reset input and side panel\n",
        "    initial_history = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello! Feel free to share anything!\"}\n",
        "    ]\n",
        "    return initial_history, \"\", \"Routing information will appear here after you send a message.\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† Compassionate AI Guide (Random Router)\")\n",
        "    gr.Markdown(\n",
        "        \"Each turn is randomly routed (50/50) to a calm style or stress-support style response, \"\n",
        "        \"then logged to CSV with routing information.\"\n",
        "    )\n",
        "\n",
        "    # Initial greeting in chat\n",
        "    initial_chat = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello! Feel free to share anything!\"}\n",
        "    ]\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500, value=initial_chat)\n",
        "\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear Chat\")\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")  # send button on the right\n",
        "\n",
        "    # Side panel for routing info\n",
        "    side_md = gr.Markdown(\"Routing information will appear here after you send a message.\")\n",
        "\n",
        "    # Wire events\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot, side_md])\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot, side_md])\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box, side_md], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, theme=gr.themes.Soft(), debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byqGU4rxXn7Y"
      },
      "source": [
        "# Version 2 (send button adjusted, reasoning shown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "ygWdoWtWZB8U",
        "outputId": "ad40d0bb-cac3-4af3-b1e1-f266eb7741d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "BlockContext.__init__() got an unexpected keyword argument 'theme'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3839088315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m )\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarm_theme\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# üß† Compassionate AI Guide\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     gr.Markdown(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, analytics_enabled, mode, title, fill_height, fill_width, delete_cache, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlocksConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BlockContext.__init__() got an unexpected keyword argument 'theme'"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# With Gemini Judge fusion (50% SVC, 50% Judge)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib, json\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\n",
        "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\n",
        "            \"ÊòØ\",\"ÁöÑ\",\"‰∫Ü\",\"Âíå\",\"Âú®\",\"Â∞±\",\"‰πü\",\"ÈÉΩ\"\n",
        "        ]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top-level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "\n",
        "    # Chinese\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging (now logs classifier, judge, fusion, reasoning)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"timestamp\",\n",
        "                \"clf_prob\",\n",
        "                \"clf_label\",\n",
        "                \"judge_prob\",\n",
        "                \"judge_label\",\n",
        "                \"judge_confidence\",\n",
        "                \"judge_reasoning\",\n",
        "                \"output_reasoning\",      # new column for output model reasoning\n",
        "                \"final_prob\",\n",
        "                \"final_label\",\n",
        "                \"user_text\",\n",
        "                \"assistant_text\",\n",
        "            ])\n",
        "\n",
        "def log_turn_to_csv(\n",
        "    user_text: str,\n",
        "    clf_prob: float,\n",
        "    clf_label: str,\n",
        "    judge_prob: float,\n",
        "    judge_label: str,\n",
        "    judge_confidence: str,\n",
        "    judge_reasoning: str,\n",
        "    output_reasoning: str,    # new param\n",
        "    final_prob: float,\n",
        "    final_label: str,\n",
        "    assistant_text: str,\n",
        "    path: str = LOG_CSV,\n",
        "):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_judge_reason = judge_reasoning.replace(\"\\n\", \" \").strip()\n",
        "    safe_output_reason = output_reasoning.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            ts,\n",
        "            round(clf_prob, 4),\n",
        "            clf_label,\n",
        "            round(judge_prob, 4),\n",
        "            judge_label,\n",
        "            judge_confidence,\n",
        "            safe_judge_reason,\n",
        "            safe_output_reason,\n",
        "            round(final_prob, 4),\n",
        "            final_label,\n",
        "            safe_user_text,\n",
        "            safe_assistant_text,\n",
        "        ])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Prompts\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "* It is ok to briefly share why you suggested a certain exercise or perspective, in friendly plain language.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "* You can briefly explain why you think certain habits or ideas might help them keep this calm state.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
        "You are a stress detection judge. Given a user's message and another model's probability P(stress), you estimate:\n",
        "- your own probability that the user is stressed,\n",
        "- a label (STRESS or CALM),\n",
        "- a confidence level,\n",
        "- and a short reasoning step by step.\n",
        "\n",
        "You MUST respond with a single line of valid JSON only, no extra text, with fields:\n",
        "{\n",
        "  \"judge_prob\": float between 0 and 1,\n",
        "  \"judge_label\": \"STRESS\" or \"CALM\",\n",
        "  \"confidence\": \"low\" or \"medium\" or \"high\",\n",
        "  \"reasoning\": \"short explanation within 50 words in friendly plain language, suitable to show to the end user\"\n",
        "}\n",
        "\n",
        "Use the user message content as the main evidence. Use the classifier probability as a weak prior only.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    model_judge = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=JUDGE_SYSTEM_PROMPT\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "def call_judge_model(user_message: str, clf_prob: float, clf_label: str):\n",
        "    \"\"\"\n",
        "    Ask Gemini judge to evaluate stress with probability and reasoning.\n",
        "    Returns: (judge_prob, judge_label, confidence, reasoning)\n",
        "    Fallbacks to classifier if parsing fails.\n",
        "    \"\"\"\n",
        "    base_prompt = f\"\"\"\n",
        "user_message: {user_message}\n",
        "classifier_prob: {clf_prob:.4f}\n",
        "classifier_label: {clf_label}\n",
        "\"\"\"\n",
        "    try:\n",
        "        resp = model_judge.generate_content(base_prompt)\n",
        "        txt = resp.text.strip()\n",
        "        # Ensure we only parse the JSON part\n",
        "        start = txt.find(\"{\")\n",
        "        end = txt.rfind(\"}\")\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            txt = txt[start:end+1]\n",
        "        data = json.loads(txt)\n",
        "\n",
        "        judge_prob = float(data.get(\"judge_prob\", clf_prob))\n",
        "        judge_prob = max(0.0, min(1.0, judge_prob))  # clamp\n",
        "        judge_label = data.get(\"judge_label\", \"STRESS\" if judge_prob >= 0.5 else \"CALM\").upper()\n",
        "        if judge_label not in (\"STRESS\", \"CALM\"):\n",
        "            judge_label = \"STRESS\" if judge_prob >= 0.5 else \"CALM\"\n",
        "        confidence = data.get(\"confidence\", \"medium\").lower()\n",
        "        if confidence not in (\"low\", \"medium\", \"high\"):\n",
        "            confidence = \"medium\"\n",
        "        reasoning = data.get(\"reasoning\", \"No reasoning provided.\")\n",
        "        return judge_prob, judge_label, confidence, reasoning\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Judge parse error, falling back to classifier:\", e)\n",
        "        # Fallback to classifier opinion\n",
        "        judge_prob = clf_prob\n",
        "        judge_label = clf_label\n",
        "        confidence = \"low\"\n",
        "        reasoning = \"Judge model output could not be parsed, so the classifier probability was used.\"\n",
        "        return judge_prob, judge_label, confidence, reasoning\n",
        "\n",
        "# 7) Gradio app (messages-only for Gradio 6)\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "def convert_gradio_to_gemini(chat_history):\n",
        "    \"\"\"\n",
        "    Gradio 6 Chatbot uses a list of dicts:\n",
        "      {\"role\": \"user\" | \"assistant\", \"content\": <str or other>}\n",
        "    Convert this into Gemini-style history with plain string parts.\n",
        "    \"\"\"\n",
        "    if not chat_history:\n",
        "        return []\n",
        "\n",
        "    gemini_history = []\n",
        "    for msg in chat_history:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        raw_content = msg.get(\"content\", \"\")\n",
        "\n",
        "        # Normalize to string\n",
        "        content = _content_to_str(raw_content)\n",
        "\n",
        "        # Strip router tag before sending to Gemini\n",
        "        if isinstance(content, str):\n",
        "            content = content.split(\"  [svc:\", 1)[0]\n",
        "\n",
        "        gemini_history.append({\n",
        "            \"role\": \"user\" if role == \"user\" else \"model\",\n",
        "            \"parts\": [content]\n",
        "        })\n",
        "    return gemini_history\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    # Gradio passes messages-format history (list[dict]) or None\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history, \"\"\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        error_msg = \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": error_msg\n",
        "        })\n",
        "        return \"\", chat_history, \"Judge reasoning not available, models not loaded.\"\n",
        "\n",
        "    # 1) SVC classifier output\n",
        "    is_stress_clf, clf_prob = predict_with_prob(message)\n",
        "    clf_label = \"STRESS\" if is_stress_clf else \"CALM\"\n",
        "\n",
        "    # 2) Gemini judge output\n",
        "    judge_prob, judge_label, judge_conf, judge_reason = call_judge_model(\n",
        "        user_message=message,\n",
        "        clf_prob=clf_prob,\n",
        "        clf_label=clf_label\n",
        "    )\n",
        "    print(f\"[judge] P(stress)={judge_prob:.4f}, label={judge_label}, conf={judge_conf}\")\n",
        "\n",
        "    # 3) 50/50 fusion\n",
        "    final_prob = 0.5 * clf_prob + 0.5 * judge_prob\n",
        "    final_is_stress = final_prob >= 0.5\n",
        "    final_label = \"STRESS\" if final_is_stress else \"CALM\"\n",
        "    print(f\"[fusion] final P(stress)={final_prob:.4f} -> {final_label}\")\n",
        "\n",
        "    # 4) Tag for display on user bubble\n",
        "    tag = (\n",
        "        f\"[svc={clf_label}; p_svc={clf_prob:.2f}; \"\n",
        "        f\"judge={judge_label}; p_judge={judge_prob:.2f}; \"\n",
        "        f\"p_final={final_prob:.2f}]\"\n",
        "    )\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [final: {final_label}]\"\n",
        "\n",
        "    # 5) Build Gemini chat history and route with fused label\n",
        "    gemini_history = convert_gradio_to_gemini(chat_history)\n",
        "    routed_model = model_stress if final_is_stress else model_calm\n",
        "    chat_session = routed_model.start_chat(history=gemini_history)\n",
        "\n",
        "    # 6) Call conversation model\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini conversation error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling the conversation model: {e}\"\n",
        "\n",
        "    # 7) Output model reasoning (separate from judge_reasoning)\n",
        "    style_word = \"supportive, stress-focused\" if final_is_stress else \"calm, positive\"\n",
        "    output_reasoning = (\n",
        "        f\"I replied in a {style_word} style because the final stress probability \"\n",
        "        f\"was {final_prob:.2f} (svc={clf_prob:.2f}, judge={judge_prob:.2f}) \"\n",
        "        f\"and the fused label was {final_label}.\"\n",
        "    )\n",
        "\n",
        "    # Assistant message with embedded reasoning for the user\n",
        "    assistant_display = (\n",
        "        f\"{response_text}\\n\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"*Model explanation:* {output_reasoning} \"\n",
        "        f\"Judge reasoning: {judge_reason}\"\n",
        "    )\n",
        "\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_display})\n",
        "\n",
        "    # 8) Log everything including judge reasoning and output reasoning\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            clf_prob=clf_prob,\n",
        "            clf_label=clf_label,\n",
        "            judge_prob=judge_prob,\n",
        "            judge_label=judge_label,\n",
        "            judge_confidence=judge_conf,\n",
        "            judge_reasoning=judge_reason,\n",
        "            output_reasoning=output_reasoning,\n",
        "            final_prob=final_prob,\n",
        "            final_label=final_label,\n",
        "            assistant_text=assistant_display,\n",
        "            path=LOG_CSV,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # 9) Prepare judge reasoning text to show in separate UI panel\n",
        "    judge_ui_text = (\n",
        "        f\"Judge label: {judge_label} (p={judge_prob:.2f}, confidence={judge_conf})\\n\"\n",
        "        f\"Reasoning: {judge_reason}\"\n",
        "    )\n",
        "\n",
        "    # Clear textbox, update chatbot and judge reasoning panel\n",
        "    return \"\", chat_history, judge_ui_text\n",
        "\n",
        "def clear_chat():\n",
        "    # Reset chatbot to initial greeting and reset input and judge panel\n",
        "    initial_history = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello! Feel free to share anything!\"}\n",
        "    ]\n",
        "    return initial_history, \"\", \"Judge reasoning will appear here after you send a message.\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† Compassionate AI Guide\")\n",
        "    gr.Markdown(\n",
        "        \"Each turn is classified by SVC, judged by Gemini, fused 50/50, \"\n",
        "        \"then routed to a calm or stress-support style. All decisions are logged to CSV.\"\n",
        "    )\n",
        "\n",
        "    # Initial greeting in chat\n",
        "    initial_chat = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello! Feel free to share anything!\"}\n",
        "    ]\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500, value=initial_chat)\n",
        "\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear Chat\")\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")  # send button on the right\n",
        "\n",
        "    # Judge reasoning area under the buttons\n",
        "    judge_md = gr.Markdown(\"Judge reasoning will appear here after you send a message.\")\n",
        "\n",
        "    # Wire events\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot, judge_md])\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot, judge_md])\n",
        "    clear_btn.click(clear_chat, None, [chatbot, msg_box, judge_md], queue=False)\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, theme=gr.themes.Soft(), debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "Q7lHhXcDOQA2",
        "outputId": "f80204fe-6910-444c-c70d-d102324cdf7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model dir: /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\n",
            "Log CSV  : /content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/logs/svc_router_logs.csv\n",
            "Loaded vectorizer and classifier.\n",
            "Decision threshold: 0.44999999999999996\n",
            "Launching app...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5e1e1c6558c2ec547e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5e1e1c6558c2ec547e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://5e1e1c6558c2ec547e.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Stress Router + Gemini Chatbot with CSV Logging (turn-by-turn)\n",
        "# With Gemini Judge fusion (50% SVC, 50% Judge)\n",
        "# ==============================================================\n",
        "\n",
        "# 0) Setup\n",
        "import os, re, csv, joblib, json\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# Try Colab Drive mount\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Note: not in Colab. Skipping Drive mount.\")\n",
        "\n",
        "# 1) Paths\n",
        "DATA_ROOT = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final\"\n",
        "MODEL_DIR = \"/content/drive/My Drive/Rice University/25fall/ELEC509/Final Project/Dataset/Stress_final/models/combined_strat/best\"\n",
        "LOG_DIR   = os.path.join(DATA_ROOT, \"logs\")\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "LOG_CSV   = os.path.join(LOG_DIR, \"svc_router_logs.csv\")\n",
        "print(\"Model dir:\", MODEL_DIR)\n",
        "print(\"Log CSV  :\", LOG_CSV)\n",
        "\n",
        "EN_SW = os.path.join(DATA_ROOT, \"english_stopwords.txt\")\n",
        "CN_SW = os.path.join(DATA_ROOT, \"chinese_stopwords.txt\")\n",
        "\n",
        "# 2) Recreate tokenizer env expected by the vectorizer\n",
        "def load_stopwords(fp: str, lowercase=False) -> set:\n",
        "    words = []\n",
        "    try:\n",
        "        with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            for ln in f:\n",
        "                ln = ln.strip()\n",
        "                if ln and not ln.startswith(\"#\"):\n",
        "                    words.append(ln.lower() if lowercase else ln)\n",
        "    except FileNotFoundError:\n",
        "        words = [\n",
        "            \"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"for\",\"on\",\"at\",\"with\",\n",
        "            \"ÊòØ\",\"ÁöÑ\",\"‰∫Ü\",\"Âíå\",\"Âú®\",\"Â∞±\",\"‰πü\",\"ÈÉΩ\"\n",
        "        ]\n",
        "    return set(words)\n",
        "\n",
        "EN_STOP = load_stopwords(EN_SW, lowercase=True)\n",
        "CN_STOP = load_stopwords(CN_SW, lowercase=False)\n",
        "\n",
        "try:\n",
        "    import jieba\n",
        "    HAS_JIEBA = True\n",
        "except Exception:\n",
        "    HAS_JIEBA = False\n",
        "\n",
        "CJK_RE = re.compile(r\"[\\u4e00-\\u9fff]\")\n",
        "TOKEN_RE_EN = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
        "FALLBACK_WORD_RE = re.compile(r\"\\w+\")\n",
        "NUM_PUNC_RE = re.compile(r\"^[\\W_]+$\")\n",
        "\n",
        "# Must be top level name to satisfy unpickling\n",
        "def mixed_tokenize(text: str):\n",
        "    text = str(text).strip()\n",
        "    toks = []\n",
        "\n",
        "    # English\n",
        "    en = [w.lower() for w in TOKEN_RE_EN.findall(text)]\n",
        "    en = [w for w in en if w not in EN_STOP]\n",
        "    toks.extend(en)\n",
        "\n",
        "    # Chinese\n",
        "    if CJK_RE.search(text):\n",
        "        if HAS_JIEBA:\n",
        "            cn = [w.strip() for w in jieba.cut(text, cut_all=False) if w.strip()]\n",
        "        else:\n",
        "            cn = [ch for ch in text if CJK_RE.match(ch)]\n",
        "        cn = [w for w in cn if w not in CN_STOP and not NUM_PUNC_RE.match(w)]\n",
        "        toks.extend(cn)\n",
        "\n",
        "    # Fallback\n",
        "    if not toks:\n",
        "        toks = [t for t in FALLBACK_WORD_RE.findall(text.lower()) if t not in EN_STOP]\n",
        "    return toks\n",
        "\n",
        "# 3) Load saved artifacts\n",
        "_VEC = None\n",
        "_CLF = None\n",
        "_THR = 0.45\n",
        "\n",
        "def _load_threshold(thr_path: str, default_thr: float = 0.45) -> float:\n",
        "    try:\n",
        "        with open(thr_path, \"r\") as f:\n",
        "            return float(f.read().strip())\n",
        "    except Exception:\n",
        "        return default_thr\n",
        "\n",
        "def load_svc_model(model_dir: str):\n",
        "    global _VEC, _CLF, _THR\n",
        "    vec_fp = os.path.join(model_dir, \"tfidf_vectorizer.joblib\")\n",
        "    clf_fp = os.path.join(model_dir, \"classifier.joblib\")\n",
        "    thr_fp = os.path.join(model_dir, \"inference_threshold.txt\")\n",
        "\n",
        "    if not os.path.exists(vec_fp):\n",
        "        raise FileNotFoundError(f\"Missing vectorizer at {vec_fp}\")\n",
        "    if not os.path.exists(clf_fp):\n",
        "        raise FileNotFoundError(f\"Missing classifier at {clf_fp}\")\n",
        "\n",
        "    _VEC = joblib.load(vec_fp)\n",
        "    _CLF = joblib.load(clf_fp)\n",
        "    _THR = _load_threshold(thr_fp, default_thr=0.45)\n",
        "\n",
        "    if not hasattr(_CLF, \"predict_proba\"):\n",
        "        raise RuntimeError(\"Classifier must be CalibratedClassifierCV to expose predict_proba.\")\n",
        "\n",
        "    print(\"Loaded vectorizer and classifier.\")\n",
        "    print(f\"Decision threshold: {_THR}\")\n",
        "\n",
        "load_svc_model(MODEL_DIR)\n",
        "\n",
        "# 4) Prediction helper that returns label and prob\n",
        "def predict_with_prob(text_input: str):\n",
        "    if _VEC is None or _CLF is None:\n",
        "        load_svc_model(MODEL_DIR)\n",
        "    X = _VEC.transform([text_input])\n",
        "    prob = float(_CLF.predict_proba(X)[0, 1])\n",
        "    is_stress = prob >= _THR\n",
        "    print(f\"[svc-v1] P(stress)={prob:.4f}, thr={_THR:.2f} -> {('STRESS' if is_stress else 'CALM')}\")\n",
        "    return bool(is_stress), prob\n",
        "\n",
        "# 5) CSV logging (now logs classifier, judge, fusion, reasoning)\n",
        "def _ensure_csv_header(path: str):\n",
        "    if not os.path.exists(path) or os.stat(path).st_size == 0:\n",
        "        with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"timestamp\",\n",
        "                \"clf_prob\",\n",
        "                \"clf_label\",\n",
        "                \"judge_prob\",\n",
        "                \"judge_label\",\n",
        "                \"judge_confidence\",\n",
        "                \"judge_reasoning\",\n",
        "                \"output_reasoning\",\n",
        "                \"final_prob\",\n",
        "                \"final_label\",\n",
        "                \"user_text\",\n",
        "                \"assistant_text\",\n",
        "            ])\n",
        "\n",
        "def log_turn_to_csv(\n",
        "    user_text: str,\n",
        "    clf_prob: float,\n",
        "    clf_label: str,\n",
        "    judge_prob: float,\n",
        "    judge_label: str,\n",
        "    judge_confidence: str,\n",
        "    judge_reasoning: str,\n",
        "    output_reasoning: str,\n",
        "    final_prob: float,\n",
        "    final_label: str,\n",
        "    assistant_text: str,\n",
        "    path: str = LOG_CSV,\n",
        "):\n",
        "    _ensure_csv_header(path)\n",
        "    ts = datetime.now(ZoneInfo(\"America/Chicago\")).isoformat(timespec=\"seconds\")\n",
        "\n",
        "    safe_user_text = user_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_assistant_text = assistant_text.replace(\"\\n\", \" \").strip()\n",
        "    safe_judge_reason = judge_reasoning.replace(\"\\n\", \" \").strip()\n",
        "    safe_output_reason = output_reasoning.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            ts,\n",
        "            round(clf_prob, 4),\n",
        "            clf_label,\n",
        "            round(judge_prob, 4),\n",
        "            judge_label,\n",
        "            judge_confidence,\n",
        "            safe_judge_reason,\n",
        "            safe_output_reason,\n",
        "            round(final_prob, 4),\n",
        "            final_label,\n",
        "            safe_user_text,\n",
        "            safe_assistant_text,\n",
        "        ])\n",
        "\n",
        "# 6) Gemini setup\n",
        "import google.generativeai as genai\n",
        "\n",
        "API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "if API_KEY is None:\n",
        "    API_KEY = \"PASTE_YOUR_GEMINI_API_KEY_HERE\"\n",
        "    if API_KEY == \"PASTE_YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise ValueError(\"Please set GOOGLE_API_KEY in Colab Secrets or as an env var.\")\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Prompts\n",
        "SYSTEM_PROMPT_STRESSFUL = \"\"\"\n",
        "**Your Role:** You are a compassionate, patient, and wise listening guide. Your goal is to help a user who is currently feeling stressed, anxious, or upset.\n",
        "\n",
        "**Context:** The user's input has been identified as \"stressful.\" Your first priority is to create a safe and calm space.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Pacify (Acknowledge and Calm)**\n",
        "* Immediately guide them with a simple, concrete calming exercise.\n",
        "* Examples: \"I hear you, that sounds very difficult. Before we talk, let's just take one deep breath together. Inhale slowly... and exhale.\" or \"Thank you for sharing. That is a heavy feeling. Let's try to ground ourselves. Can you look around and name one thing you see that is blue?\"\n",
        "* After the exercise, acknowledge their feelings.\n",
        "\n",
        "**Step 2: Analyze and Guide (The Six Categories)**\n",
        "* Listen to their problem. Gently analyze if their suffering might be related to one of the following six unhelpful mind-states.\n",
        "* Do not use the technical terms. Instead, identify the pattern and guide them away using the tools provided.\n",
        "    * If craving: clinging, excessive desire, \"I must have...\"\n",
        "        * Tool: use an analogy about impermanence or a verse about contentment.\n",
        "    * If aversion or anger: blame, hatred, resentment.\n",
        "        * Tool: use a story about forgiveness or an analogy like \"holding anger is like holding a hot coal.\"\n",
        "    * If confusion: feeling lost, no direction.\n",
        "        * Tool: focus on truth and clarity, break the problem into small true pieces.\n",
        "    * If comparison or conceit: \"I am better\" or \"I am worse.\"\n",
        "        * Tool: describe a beautiful scene (a forest where every tree is different but essential).\n",
        "    * If doubt: paralyzing skepticism, lack of trust.\n",
        "        * Tool: use a simple truth or verse, encourage one small step.\n",
        "    * If rigid negative belief: \"I am worthless.\"\n",
        "        * Tool: use a story or rare event that gives a different perspective.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* After offering guidance, conclude with a supportive, open-ended question.\n",
        "* It is ok to briefly share why you suggested a certain exercise or perspective, in friendly plain language.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_CALM = \"\"\"\n",
        "**Your Role:** You are a positive, encouraging, and wise companion.\n",
        "\n",
        "**Context:** The user's input has been identified as calm. Your goal is to reinforce this positive state and provide tools for maintaining it.\n",
        "\n",
        "**Your 3-Step Process:**\n",
        "\n",
        "**Step 1: Compliment and Reinforce**\n",
        "* Begin by genuinely acknowledging and complimenting their positive state.\n",
        "\n",
        "**Step 2: Follow Their Interest**\n",
        "* Follow their topics. Explore what they find interesting or joyful.\n",
        "* You can briefly explain why you think certain habits or ideas might help them keep this calm state.\n",
        "\n",
        "**Step 3: Conclude and Ask**\n",
        "* Conclude with an open-ended question that invites further discussion.\n",
        "\"\"\"\n",
        "\n",
        "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
        "You are a stress detection judge. Given a user's message and another model's probability P(stress), you estimate:\n",
        "- your own probability that the user is stressed,\n",
        "- a label (STRESS or CALM),\n",
        "- a confidence level,\n",
        "- and a short reasoning step by step.\n",
        "\n",
        "You MUST respond with a single line of valid JSON only, no extra text, with fields:\n",
        "{\n",
        "  \"judge_prob\": float between 0 and 1,\n",
        "  \"judge_label\": \"STRESS\" or \"CALM\",\n",
        "  \"confidence\": \"low\" or \"medium\" or \"high\",\n",
        "  \"reasoning\": \"short explanation within 50 words in friendly plain language, suitable to show to the end user\"\n",
        "}\n",
        "\n",
        "Use the user message content as the main evidence. Use the classifier probability as a weak prior only.\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    model_stress = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_STRESSFUL\n",
        "    )\n",
        "    model_calm = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=SYSTEM_PROMPT_CALM\n",
        "    )\n",
        "    model_judge = genai.GenerativeModel(\n",
        "        model_name=\"gemini-2.5-pro\",\n",
        "        system_instruction=JUDGE_SYSTEM_PROMPT\n",
        "    )\n",
        "    MODELS_LOADED = True\n",
        "except Exception as e:\n",
        "    MODELS_LOADED = False\n",
        "    print(f\"FATAL: could not initialize Gemini models: {e}\")\n",
        "\n",
        "# 7) Gradio app (messages-only for Gradio 6)\n",
        "import gradio as gr\n",
        "\n",
        "def _content_to_str(content):\n",
        "    \"\"\"Normalize Gradio Chatbot content (str, list, dict) to a plain string.\"\"\"\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for c in content:\n",
        "            if isinstance(c, dict) and \"text\" in c:\n",
        "                parts.append(str(c[\"text\"]))\n",
        "            else:\n",
        "                parts.append(str(c))\n",
        "        return \" \".join(parts)\n",
        "    if isinstance(content, dict) and \"text\" in content:\n",
        "        return str(content[\"text\"])\n",
        "    return str(content)\n",
        "\n",
        "SHOW_PROB = True\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    # Gradio passes messages-format history (list[dict]) or None\n",
        "    if chat_history is None:\n",
        "        chat_history = []\n",
        "\n",
        "    # Block empty messages\n",
        "    if not message or not str(message).strip():\n",
        "        return \"\", chat_history, \"\"\n",
        "\n",
        "    if not MODELS_LOADED:\n",
        "        chat_history = list(chat_history)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        error_msg = \"Error: AI models could not be loaded. Check API key and configuration.\"\n",
        "        chat_history.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": error_msg\n",
        "        })\n",
        "        return \"\", chat_history, \"Judge reasoning not available, models not loaded.\"\n",
        "\n",
        "    # 1) SVC classifier output\n",
        "    is_stress_clf, clf_prob = predict_with_prob(message)\n",
        "    clf_label = \"STRESS\" if is_stress_clf else \"CALM\"\n",
        "\n",
        "    # 2) Gemini judge output\n",
        "    judge_prob, judge_label, judge_conf, judge_reason = call_judge_model(\n",
        "        user_message=message,\n",
        "        clf_prob=clf_prob,\n",
        "        clf_label=clf_label\n",
        "    )\n",
        "    print(f\"[judge] P(stress)={judge_prob:.4f}, label={judge_label}, conf={judge_conf}\")\n",
        "\n",
        "    # 3) 50/50 fusion\n",
        "    final_prob = 0.5 * clf_prob + 0.5 * judge_prob\n",
        "    final_is_stress = final_prob >= 0.5\n",
        "    final_label = \"STRESS\" if final_is_stress else \"CALM\"\n",
        "    print(f\"[fusion] final P(stress)={final_prob:.4f} -> {final_label}\")\n",
        "\n",
        "    # 4) Tag for display on user bubble\n",
        "    tag = (\n",
        "        f\"[svc={clf_label}; p_svc={clf_prob:.2f}; \"\n",
        "        f\"judge={judge_label}; p_judge={judge_prob:.2f}; \"\n",
        "        f\"p_final={final_prob:.2f}]\"\n",
        "    )\n",
        "    display_user = f\"{message}  {tag}\" if SHOW_PROB else f\"{message}  [final: {final_label}]\"\n",
        "\n",
        "    # 5) Route to stress or calm model, with fresh history every time\n",
        "    routed_model = model_stress if final_is_stress else model_calm\n",
        "    chat_session = routed_model.start_chat(history=[])\n",
        "\n",
        "    # 6) Call conversation model\n",
        "    try:\n",
        "        response = chat_session.send_message(message)\n",
        "        response_text = response.text\n",
        "    except Exception as e:\n",
        "        print(\"Gemini conversation error:\", e)\n",
        "        response_text = f\"Sorry, an error occurred when calling the conversation model: {e}\"\n",
        "\n",
        "    # 7) Output model reasoning (separate from judge_reasoning)\n",
        "    style_word = \"supportive, stress-focused\" if final_is_stress else \"calm, positive\"\n",
        "    output_reasoning = (\n",
        "        f\"I replied in a {style_word} style because the final stress probability \"\n",
        "        f\"was {final_prob:.2f} (svc={clf_prob:.2f}, judge={judge_prob:.2f}) \"\n",
        "        f\"and the fused label was {final_label}.\"\n",
        "    )\n",
        "\n",
        "    # Assistant message with embedded reasoning for the user\n",
        "    assistant_display = (\n",
        "        f\"{response_text}\\n\\n\"\n",
        "        \"---\\n\"\n",
        "        f\"*Model explanation:* {output_reasoning} \"\n",
        "        f\"Judge reasoning: {judge_reason}\"\n",
        "    )\n",
        "\n",
        "    chat_history = list(chat_history)\n",
        "    chat_history.append({\"role\": \"user\", \"content\": display_user})\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_display})\n",
        "\n",
        "    # 8) Log everything including judge reasoning and output reasoning\n",
        "    try:\n",
        "        log_turn_to_csv(\n",
        "            user_text=message,\n",
        "            clf_prob=clf_prob,\n",
        "            clf_label=clf_label,\n",
        "            judge_prob=judge_prob,\n",
        "            judge_label=judge_label,\n",
        "            judge_confidence=judge_conf,\n",
        "            judge_reasoning=judge_reason,\n",
        "            output_reasoning=output_reasoning,\n",
        "            final_prob=final_prob,\n",
        "            final_label=final_label,\n",
        "            assistant_text=assistant_display,\n",
        "            path=LOG_CSV,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] logging failed: {e}\")\n",
        "\n",
        "    # 9) Prepare judge reasoning text to show in separate UI panel\n",
        "    judge_ui_text = (\n",
        "        f\"Judge label: {judge_label} (p={judge_prob:.2f}, confidence={judge_conf})\\n\"\n",
        "        f\"Reasoning: {judge_reason}\"\n",
        "    )\n",
        "\n",
        "    # Clear textbox, update chatbot and judge reasoning panel\n",
        "    return \"\", chat_history, judge_ui_text\n",
        "\n",
        "# 8) UI layout with warm theme passed at launch\n",
        "warm_theme = gr.themes.Soft(\n",
        "    primary_hue=\"yellow\",   # golden\n",
        "    secondary_hue=\"purple\"  # saturated purple\n",
        ")\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üß† Compassionate AI Guide\")\n",
        "    gr.Markdown(\n",
        "        \"Each turn is classified by SVC, judged by Gemini, fused 50/50, \"\n",
        "        \"then routed to a calm or stress-support style. All decisions are logged to CSV.\"\n",
        "    )\n",
        "\n",
        "    # Initial greeting\n",
        "    initial_chat = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello! Feel free to share anything!\"}\n",
        "    ]\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", height=500, value=initial_chat)\n",
        "\n",
        "    msg_box = gr.Textbox(label=\"Your message\", placeholder=\"How are you feeling?\")\n",
        "\n",
        "    # Send button on the right using Columns\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            gr.HTML(\"\")  # spacer\n",
        "        with gr.Column(scale=1):\n",
        "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "    # Judge reasoning panel\n",
        "    judge_md = gr.Markdown(\"Judge reasoning will appear here after you send a message.\")\n",
        "\n",
        "    # Wiring\n",
        "    msg_box.submit(respond, [msg_box, chatbot], [msg_box, chatbot, judge_md])\n",
        "    send_btn.click(respond, [msg_box, chatbot], [msg_box, chatbot, judge_md])\n",
        "\n",
        "print(\"Launching app...\")\n",
        "demo.launch(share=True, debug=True, theme=warm_theme)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}